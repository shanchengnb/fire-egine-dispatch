{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "import ast\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from fire_dispatch_rl_env.environment import FireDispatchEnv\n",
    "from fire_dispatch_rl_env.wrappers import WrappedDispatchEnv\n",
    "\n",
    "# === üì¶ 0. Download and extract data files ===\n",
    "zip_url = \"https://github.com/shanchengnb/fire-dispatch/raw/refs/heads/master/final%20data.zip\"\n",
    "zip_path = \"final_data.zip\"\n",
    "extract_dir = \"final_data\"\n",
    "\n",
    "if not os.path.exists(zip_path):\n",
    "    print(\"‚¨áÔ∏è Downloading data package...\")\n",
    "    response = requests.get(zip_url)\n",
    "    with open(zip_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "if not os.path.exists(extract_dir):\n",
    "    print(\"üì¶ Extracting data package...\")\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(extract_dir)\n",
    "\n",
    "# === üîç File search utility ===\n",
    "def find_file_by_keyword(folder, keyword):\n",
    "    for root, _, files in os.walk(folder):\n",
    "        for f in files:\n",
    "            if keyword.lower() in f.lower():\n",
    "                return os.path.join(root, f)\n",
    "    raise FileNotFoundError(f\"‚ùå File containing '{keyword}' not found\")\n",
    "# === üìÅ 1. Load data ===\n",
    "csv_path = find_file_by_keyword(extract_dir, \"real_incidents\")\n",
    "df = pd.read_csv(csv_path)\n",
    "if isinstance(df.loc[0, 'graph_node'], str):\n",
    "    df[\"graph_node\"] = df[\"graph_node\"].apply(ast.literal_eval)\n",
    "\n",
    "# === Load .pkl files ===\n",
    "def load_pickle(path):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"‚ùå File not found: {path}\")\n",
    "    with open(path, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "station_dists = load_pickle(\"station_dists.pkl\")\n",
    "station_mapping = load_pickle(\"station_mapping.pkl\")\n",
    "\n",
    "# ‚úÖ Read CSV correctly as DataFrame, then convert to dict\n",
    "counts_path = find_file_by_keyword(extract_dir, \"station_engine_counts\")\n",
    "df_counts = pd.read_csv(counts_path)\n",
    "station_engine_counts = dict(zip(df_counts[\"Station name\"], df_counts[\"station_engine_counts\"]))\n",
    "\n",
    "station_xy = {}\n",
    "xy_path = \"station_xy.pkl\"\n",
    "if os.path.exists(xy_path):\n",
    "    station_xy = load_pickle(xy_path)\n",
    "\n",
    "# === ‚öôÔ∏è 2. Environment configuration ===\n",
    "config = {\n",
    "    \"max_engines\": len(station_mapping),\n",
    "    \"cooldown_seconds\": 180,\n",
    "    \"event_num\": 300,\n",
    "    \"max_steps\": 300,\n",
    "    \"map_width\": 50000,\n",
    "    \"map_height\": 50000,\n",
    "    \"obs_dim\": 96,\n",
    "    \"obs_engine_count\": 20,\n",
    "    \"station_dists\": station_dists,\n",
    "    \"station_engine_counts\": station_engine_counts,\n",
    "    \"station_xy\": station_xy,\n",
    "    \"average_speed_kmph\": 48\n",
    "}\n",
    "\n",
    "# === üß± 3. Create wrapped environment (for fixed action space) ===\n",
    "base_env = FireDispatchEnv(config, event_df=df)\n",
    "env = WrappedDispatchEnv(base_env, max_actions=20)\n",
    "\n",
    "obs = env.reset()\n",
    "print(f\"‚úÖ obs shape: {np.shape(obs)} | type: {type(obs)}\")\n",
    "\n",
    "# === üíæ 4. Save path and checkpoint callback ===\n",
    "save_path = \"models/fire_dqn_final\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq=10_000,\n",
    "    save_path=save_path,\n",
    "    name_prefix=\"checkpoint\",\n",
    "    save_replay_buffer=True,\n",
    "    save_vecnormalize=True\n",
    ")\n",
    "\n",
    "# === üîß 5. Create reinforcement learning model ===\n",
    "model = DQN(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=env,\n",
    "    learning_rate=1e-4,\n",
    "    buffer_size=100_000,\n",
    "    learning_starts=1000,\n",
    "    batch_size=64,\n",
    "    tau=0.1,\n",
    "    gamma=0.99,\n",
    "    train_freq=4,\n",
    "    target_update_interval=1000,\n",
    "    exploration_fraction=0.2,\n",
    "    exploration_final_eps=0.05,\n",
    "    verbose=1,\n",
    "    tensorboard_log=\"./tensorboard/\",\n",
    "    policy_kwargs=dict(\n",
    "        net_arch=[128, 128],  # Enhance representation power\n",
    "            # ‚úÖ Enable Duelling DQN\n",
    "    )\n",
    ")\n",
    "\n",
    "# === üöÄ 6. Start training ===\n",
    "model.learn(\n",
    "    total_timesteps=200_000,\n",
    "    callback=checkpoint_callback\n",
    ")\n",
    "\n",
    "# === ‚úÖ 7. Save the final model ===\n",
    "model.save(os.path.join(save_path, \"final_model\"))\n",
    "print(\"‚úÖ Model training complete and saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from fire_dispatch_rl_env.environment import FireDispatchEnv\n",
    "from fire_dispatch_rl_env.wrappers import WrappedDispatchEnv\n",
    "\n",
    "# === üìÅ 1. Load data ===\n",
    "csv_path = r\"D:\\UCL2\\final paper\\data\\real_incidents_with_station_info_with_on_scene_seconds.csv\"# ‚ö†Ô∏è This is a local file path, please ensure the file exists at this location on your machine\n",
    "df = pd.read_csv(csv_path)\n",
    "if isinstance(df.loc[0, 'graph_node'], str):\n",
    "    df[\"graph_node\"] = df[\"graph_node\"].apply(ast.literal_eval)\n",
    "\n",
    "with open(\"station_dists.pkl\", \"rb\") as f:\n",
    "    station_dists = pickle.load(f)\n",
    "with open(\"station_mapping.pkl\", \"rb\") as f:\n",
    "    station_mapping = pickle.load(f)\n",
    "\n",
    "station_xy = {}\n",
    "xy_path = r\"D:\\UCL2\\final paper\\data\\station_xy.pkl\"\n",
    "if os.path.exists(xy_path):\n",
    "    with open(xy_path, \"rb\") as f:\n",
    "        station_xy = pickle.load(f)\n",
    "\n",
    "# ‚úÖ Directly specify path to station_engine_counts file\n",
    "counts_path = r\"D:\\UCL2\\final paper\\data\\Station_engine_counts.csv\"\n",
    "df_counts = pd.read_csv(counts_path)\n",
    "station_engine_counts = dict(zip(df_counts[\"Station name\"], df_counts[\"station_engine_counts\"]))\n",
    "\n",
    "# === ‚öôÔ∏è 2. Environment configuration ===\n",
    "config = {\n",
    "    \"max_engines\": len(station_mapping),\n",
    "    \"cooldown_seconds\": 180,\n",
    "    \"event_num\": 300,\n",
    "    \"max_steps\": 300,\n",
    "    \"map_width\": 50000,\n",
    "    \"map_height\": 50000,\n",
    "    \"obs_dim\": 96,\n",
    "    \"obs_engine_count\": 20,\n",
    "    \"station_dists\": station_dists,\n",
    "    \"station_engine_counts\": station_engine_counts,\n",
    "    \"station_xy\": station_xy,\n",
    "    \"average_speed_kmph\": 48\n",
    "}\n",
    "\n",
    "# === üß± 3. Create wrapped environment (for stable action space) ===\n",
    "base_env = FireDispatchEnv(config, event_df=df)\n",
    "env = WrappedDispatchEnv(base_env, max_actions=20)\n",
    "\n",
    "obs = env.reset()\n",
    "print(f\"‚úÖ obs shape: {np.shape(obs)} | type: {type(obs)}\")\n",
    "\n",
    "# === üíæ 4. Save path and checkpoint callback ===\n",
    "save_path = \"models/fire_dqn_final\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq=10_000,\n",
    "    save_path=save_path,\n",
    "    name_prefix=\"checkpoint\",\n",
    "    save_replay_buffer=True,\n",
    "    save_vecnormalize=True\n",
    ")\n",
    "\n",
    "# === üîß 5. Create reinforcement learning model ===\n",
    "model = DQN(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=env,\n",
    "    learning_rate=1e-4,\n",
    "    buffer_size=100_000,\n",
    "    learning_starts=1000,\n",
    "    batch_size=64,\n",
    "    tau=0.1,\n",
    "    gamma=0.99,\n",
    "    train_freq=4,\n",
    "    target_update_interval=1000,\n",
    "    exploration_fraction=0.2,\n",
    "    exploration_final_eps=0.05,\n",
    "    verbose=1,\n",
    "    tensorboard_log=\"./tensorboard/\",\n",
    "    policy_kwargs=dict(\n",
    "        net_arch=[128, 128],  # Enhance representation power\n",
    "    )\n",
    ")\n",
    "\n",
    "# === üöÄ 6. Start training ===\n",
    "model.learn(\n",
    "    total_timesteps=200_000,\n",
    "    callback=checkpoint_callback\n",
    ")\n",
    "\n",
    "# === ‚úÖ 7. Save final model ===\n",
    "model.save(os.path.join(save_path, \"final_model\"))\n",
    "print(\"‚úÖ Model training complete and saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "\n",
    "from fire_dispatch_rl_env.environment import FireDispatchEnv\n",
    "from fire_dispatch_rl_env.wrappers import ContinuousDispatchEnv\n",
    "\n",
    "# === üìÅ 1. Load data ===\n",
    "# ‚ö†Ô∏è This is a local file path, please ensure the file exists at this location on your machine\n",
    "csv_path = r\"D:\\UCL2\\final paper\\data\\real_with_dispatch_info.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "if isinstance(df.loc[0, 'graph_node'], str):\n",
    "    df[\"graph_node\"] = df[\"graph_node\"].apply(ast.literal_eval)\n",
    "\n",
    "with open(\"station_dists.pkl\", \"rb\") as f:\n",
    "    station_dists = pickle.load(f)\n",
    "with open(\"station_mapping.pkl\", \"rb\") as f:\n",
    "    station_mapping = pickle.load(f)\n",
    "\n",
    "station_xy = {}\n",
    "xy_path = r\"D:\\UCL2\\final paper\\data\\station_xy.pkl\"\n",
    "if os.path.exists(xy_path):\n",
    "    with open(xy_path, \"rb\") as f:\n",
    "        station_xy = pickle.load(f)\n",
    "\n",
    "counts_path = r\"D:\\UCL2\\final paper\\data\\Station_engine_counts.csv\"\n",
    "df_counts = pd.read_csv(counts_path)\n",
    "station_engine_counts = dict(zip(df_counts[\"Station name\"], df_counts[\"station_engine_counts\"]))\n",
    "\n",
    "# === ‚öôÔ∏è 2. Environment configuration ===\n",
    "config = {\n",
    "    \"max_engines\": len(station_mapping),\n",
    "    \"cooldown_seconds\": 180,\n",
    "    \"event_num\": 150,             # ‚úÖ Reduce number of events\n",
    "    \"max_steps\": 150,             # ‚úÖ Reduce max steps (avoid waste)\n",
    "    \"map_width\": 50000,\n",
    "    \"map_height\": 50000,\n",
    "    \"obs_dim\": 96,\n",
    "    \"obs_engine_count\": 20,\n",
    "    \"station_dists\": station_dists,\n",
    "    \"station_engine_counts\": station_engine_counts,\n",
    "    \"station_xy\": station_xy,\n",
    "    \"average_speed_kmph\": 48,\n",
    "    \"max_dispatch_per_event\": 2  # ‚úÖ Multi-vehicle dispatch support\n",
    "}\n",
    "\n",
    "# === üåç 3. Initialize continuous action environment ===\n",
    "base_env = FireDispatchEnv(config, event_df=df)\n",
    "env = ContinuousDispatchEnv(base_env)\n",
    "\n",
    "obs = env.reset()\n",
    "print(f\"‚úÖ Continuous action environment initialized, obs shape: {np.shape(obs)}\")\n",
    "\n",
    "# === üíæ 4. Save path and callback setup ===\n",
    "save_path = \"models/fire_sac_debug_10w\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq=5000,\n",
    "    save_path=save_path,\n",
    "    name_prefix=\"checkpoint\",\n",
    "    save_replay_buffer=True,\n",
    "    save_vecnormalize=True\n",
    ")\n",
    "\n",
    "# === üîß 5. Build SAC model (continuous actions) ===\n",
    "model = SAC(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=env,\n",
    "    learning_rate=3e-4,\n",
    "    buffer_size=100_000,\n",
    "    learning_starts=1000,\n",
    "    batch_size=128,\n",
    "    tau=0.005,\n",
    "    gamma=0.99,\n",
    "    train_freq=1,\n",
    "    gradient_steps=1,\n",
    "    verbose=1,\n",
    "    tensorboard_log=\"./tensorboard/\",\n",
    "    policy_kwargs=dict(net_arch=[256, 256])\n",
    ")\n",
    "\n",
    "# === üöÄ 6. Start training (only 100k steps) ===\n",
    "model.learn(\n",
    "    total_timesteps=100_000,      # ‚úÖ Limit to 100k steps\n",
    "    callback=checkpoint_callback\n",
    ")\n",
    "\n",
    "# === ‚úÖ 7. Save final model ===\n",
    "model.save(os.path.join(save_path, \"final_model713\"))\n",
    "print(\"‚úÖ SAC multi-vehicle dispatch training complete (100k steps), model saved!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
